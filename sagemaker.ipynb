{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sagemaker.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOATdPLqS1oiF5FlQLLWhSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bugatha1/sagemaker/blob/main/sagemaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "boto3 library is a python sdk for interacting with aws cli and various aws services."
      ],
      "metadata": {
        "id": "qhNGLC0ewtfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several options to traing algorithms in sagemaker.<br>\n",
        "1) Built in algorithms : No custom code or least customcode required to train models. we need specify s3 path of training data aswellas training job configuration, sagemaker takes care of the rest. <br>\n",
        "2) Custom code : This option allow us to create custom training scripts to run using prebuilt docker images.<br>\n",
        "\n",
        "3) Custom Algorithms: Rather than using prebuilt images we can package our code in custom docker image and specify the registery path of the image in the training job. This is the most flexible option"
      ],
      "metadata": {
        "id": "M2ptH2klymoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create training jobs using amazon sagemaker console or the api. If we create training job with api, use either sagemaker python sdk or the boto3 library. Let's use sagemaker python sdk,  then create sagemaker session object, then session object manage interactions between sagemaker apis and other aws services if any needed. Aws service calls delegate to underline boto sesion that created."
      ],
      "metadata": {
        "id": "nn1JE_J37Yuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After data preprocessing, we will get train and test data sets. <br>\n",
        "First we need to upload the training dataset to s3 using sagemaker session object.<br> After uploaded data into s3, then create a channel configuration that are our training function can use as pointer to those files in s3. <br>\n",
        "After uploaded data, then we need to train the model. To train the model we need to create training jobs. Training job includes following information.<br>\n",
        "-> s3 bucket url which contains training data.<br>\n",
        "-> compute resources that we are using for training. <br>\n",
        "-> url need to provide to store output of job. (we can choose same s3 bucket also. This output job contains artifacts, serialized version of changed models, checkpoints, metrics) <br>\n",
        "-> ECR (Elastic contrainer registry) path which points to the docker image that will be using actual training path."
      ],
      "metadata": {
        "id": "H1Vq6xtW9WeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train the model we are going to create instance of the sagemaker estimator class and provide image name(docker image name), role (aws iam role),\n",
        "train instance count and train instance type, output path, base job name and sagemaker session. Next call sethyperparameter method to set the hyperparameters to the model.\n"
      ],
      "metadata": {
        "id": "8zV_oHMKGj_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In CustomCode algorithm typically the training script loads data from input channels configure training with hyperparameters, trains model and saved model to the directory that can be hosted later. Hyperparameters can pass to script as command line arguments. Some default arguments provided by environment variables defined in the sagemaker docker images."
      ],
      "metadata": {
        "id": "TFyqnt5KqBPM"
      }
    }
  ]
}